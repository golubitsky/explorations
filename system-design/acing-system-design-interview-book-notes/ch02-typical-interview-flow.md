- **Clarify System Requirements. Discuss Possible Tradeoffs**

  - Quickly brainstorm and scribble down potential functional requirements
  - 30 seconds: Overall purpose of the system
  - Briefly mention endpoints common to most systems, like `/health`, `/signup`, `/login`
  - **Details of Common Functional Requirements**
    - **User Categories/Roles**
      - Who will use the system and how?
        - Manual/consumer: mobile, web apps
        - Programmatic/enterprise: requests from other services/companies
      - Technical vs non-technical
      - List user roles, e.g., buyer, seller, poster, viewer
      - Every functional requirement must have a number
        - How many items
        - How much time
      - Communication between users and/or operations staff
      - Regionalization (i18n and l10n)
        - Languages
        - Postal address
        - Prices — multiple currency support
    - **Scalability Requirements Based on User Categories**
      - Estimate daily active users
      - Estimate daily/hourly request rate
        - E.g., 1 billion daily users; each submitting 10 search requests; 10B daily requests, 420M req/H
    - **Data Access**
      - Which data should be accessible to which users
      - Authentication/authorization roles and mechanisms
      - Contents of the response body for API content
      - Data retrieval frequency (e.g., real-time, monthly reports)
    - **Search**
      - Possible use cases that involve search
    - **Analytics**
      - Possible machine learning requirements
        - Support for experimentation such as A/B testing, multi-armed bandit
    - **PseudoCode Function Signatures**
      - E.g., `fetchPosts`
      - Match them to user stories
      - Discuss with interviewer which requirements are needed and which are out of scope
      - Ask, "Are there other user requirements?"
        - Do the thinking yourself
        - Requirements are subtle
        - Ask clarifying questions
        - Brainstorm future enhancements
        - Demonstrate critical thinking, attention to detail, humility, willingness to learn
    - **Non-Functional Requirements**
      - Design immediately for scalability?
        - If not, focus on functional requirements, e.g., data model and APIs

- **Draft the API Specification**

  - Based on functional requirements, determine the data that users expect to receive from and send to the system
  - **< 5 minutes:** Draft REST endpoints, including path, query params
    - Don't linger here; "there's much more to discuss in limited time"
    - Don't update functional requirements here
  - Propose the API spec and describe how it satisfies all functional requirements
    - Briefly describe any missed functional requirements
  - **Common API Endpoints** (likely out of scope, quickly mention)
    - `GET /health`
    - `POST /signup`
      - OpenID Connect (OIDC): Token-based authentication performed by authorization server
    - User management

- **Connections and Processing Between Users and Data**

  - **Phase 1**
    - Draw a box to represent class of user
    - Draw a box to represent each system that serves the functional requirements
    - Draw connections between users and systems
  - **Phase 2**
    - Break up request processing and storage
    - Create different designs based on non-functional requirements, e.g., real-time vs eventual consistency
    - Consider shared services
  - **Phase 3**
    - Break up systems into components, e.g., libraries, services
    - Draw connections
    - Consider logging, monitoring, alerting
    - Consider security
  - **Phase 4**
    - Include a summary of system design
    - Provide any new additional requirements
    - Analyze fault tolerance
      - What can go wrong with each component?
        - Network delays
        - Inconsistency
        - No linearizability
      - What can we do to prevent and/or mitigate each situation to improve fault tolerance of component and overall system?

- **Design the Data Model. Discuss Possible Analytics**

- **Discuss Failure Design and Graceful Degradation**

  - **Levels of Urgency of Failures**
    - If the service is a dependency of other services, maybe higher urgency
  - **Observability: Logging, Monitoring, Alerting**
    - **Never forget to mention monitoring!**
    - Refer to Google's SRE book for 4 golden signals of monitoring:
      - Latency — Alerts for latency exceeding SLA
      - Traffic — Alerts for higher traffic than supported by load testing
      - Errors — High-urgency alerts for 500s and select 400s that must be addressed urgently
      - Saturation — Depending on system constraints (CPU, memory, I/O), set up utilization targets that should not be exceeded; file or DB usage may run out
    - **Instruments of Monitoring:**
      - Metrics (a variable we can measure)
      - Dashboards
      - Alerts
    - **Logging**
      - **General Considerations**
        - Structured, machine-readable
        - Each entry should contain UUID to support distributed tracing and user-dev communication
        - Small, easy to read, useful
        - Consistent format, e.g., timestamps
        - Log level
        - Do not log private/sensitive information (PII, personally identifiable information)
      - **Host Logging**
        - CPU/mem utilization
        - Network I/O
      - **Request-Level Logging**
        - Latency
        - Source of request
        - Request path, params, headers, body
        - HTTP status code and response body
        - Errors
        - Log events to monitor how well the system satisfies both functional and non-functional requirements
          - E.g., if we build a cache, monitor log cache hits, misses, page faults (not in memory)
            - Metrics would include counts of each
      - **Batch/Streaming Audit Jobs**
        - Run periodically to validate system data integrity, possibly triggering alerts
    - **Responding to Alerts**
      - Team may set up an on-call schedule for high-urgency alerts
      - On-call engineer may not be familiar with the cause of particular alerts
      - Create a runbook with list of alerts, possible causes, procedures to find and fix issues
        - If certain instructions consist of a series of commands that can easily be copied and pasted
          - Automate
        - Failure to implement such automation is runbook abuse
        - If certain instructions are to run commands to view metrics
          - Display those metrics on a dashboard
      - Prometheus (monitoring system) + Grafana (visualization)
    - Streaming and batch audit of data quality

- **Discuss Complexity and Tradeoffs, Maintenance and Decommissioning Processes, Costs**

  - **Extend the Service to Support Other Types of Users**
    - Extend current services
    - Build new services
    - Tradeoffs
  - **Alternative Architectural Decisions**
    - Revisit earlier-mentioned alternatives in more detail
  - **Usability Metrics**

    - Collect use-case specific metrics to influence product development
      - E.g., for a search engine, how close to the top of the results were users finding what they needed
      - Number of help desk tickets created per week (we want self-service)
    - UI components to solicit feedback from users

  - **Search Bar**
    - **Elasticsearch**
      - Loose mapping of terms from SQL DB to ES:
        - DB: index
        - Partition: shard
        - Table: type (deprecated without replacement)
        - Column: field
        - Row: document
        - Schema: mapping
        - Index: everything is indexed
      - **Query Context**
        - "How well does the document match the query clause?" — relevance score
      - **Filter Context**
        - "Does this document match the query clause?" — yes or no

- **Reflect on the Interview**
- **Evaluating the Company**

  - Before your interview, read blogs to learn:
    - What is this tool?
    - Who uses it?
    - What does it do? How does it do these things? How is it different from other tools? What can it do that other tools cannot do? What can't it do that other tools can do?
  - From each article, write down two questions to ask
  - **Points to Understand, in General:**
    - Technology stack
    - Data tools and infrastructure
    - Which tools were bought vs developed?
    - Which open-source contributions has the company made?
    - History and development of various engineering projects
    - Quantity/breakdown of engineering resources
    - How well did the tools address the users' requirements?
      - Best experiences and pain points
      - Which tools were abandoned and why?
      - Comparison to competitors and to state of the art
      - What has the company done to address these points?
    - Engineers' experience with CI/CD tools
      - How often run into problems
      - How long to troubleshoot
    - What projects are planned and what needs do they fulfill?
    - What is the engineering org's strategic vision?
    - Org-wide migration in the last two years
      - E.g., shift from bare metal to cloud vendor; stop using various tools
      - Have there been U-turns in migrations? What motivated the U-turns?
    - Security breaches in the history of the company?
    - What can I learn and cannot learn from this company in the next four years?
    - Blog on reverse interviewing

- **Summary**
  - Everything is a tradeoff
  - Be mindful of time. Clarify important points and focus on them
  - Clarify the system requirements and discuss possible tradeoffs in the system's capabilities to optimize for them
  - Draft the API specification to satisfy the functional requirements
  - Draw connections between users and data (read and write)
  - Other concerns: logging, monitoring, alerting, search, and anything else that comes up
