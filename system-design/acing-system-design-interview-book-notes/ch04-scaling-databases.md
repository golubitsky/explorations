- **Storage services**

  - Compared to stateless services, stateful services have mechanisms to ensure consistency and require redundancy to avoid data loss.
    - examples of mechanisms:
      - Paxos for strong consistency
      - eventual-consistency
    - tradeoffs have to be made, depending on requirements for
      - consistency
      - complexity
      - security
      - latency
      - performance
    - strong consistency
      - only one consistent state
      - all accesses are seen by all processes in the same order
    - weak consistency
      - multiple states
      - processes can read data in different order than modified
  - **Motivation for use**
    - Keep as many services stateless, to avoid this complexity. Other reasons:
      - if individual host has state, we need to implement sticky sessions (consistently route same user to same host)
      - need to replicate data in case host fails, and handle failover
    - By pushing state to a stateful storage device, we can choose the appropriate storage/DB technology and avoid designing state management
  - **Categories**
    - Database
      - SQL: relational, primary/foreign keys, ACID transactions
      - NoSQL: does not have all SQL properties
      - Column-oriented: data stored in columns instead of rows, for efficient access.
        - examples: HBase, Cassandra
      - Key-value: usually used for caching using various techniques like LRU
        - has high performance but does not require high availability
        - examples: Memcached, Redis, AWS Elasticache
    - Document: a key-value DB with no or much larger size limits
      - example: MongoDB
    - Graph: designed to efficiently store relationships between entities
      - examples: Neo4j, RedisGraph, Amazon Neptune
    - File storage: store data in files/directories; a form of key-value, where key=path
    - Block storage: store data in evenly sized chunks w/ unique IDs
      - unlikely to use in web apps
      - relevant for designing lower level components of other storage systems (like databases)
    - Object storage
      - flatter hierarchy than file storage
      - objects usually accessed via HTTP APIs
      - slow writes and objects can't be modified
      - suited for static data
      - example: AWS S3
  - **When to use vs. avoid databases**
    - Based on object/file size
      - less than 256K -> database
      - 256K - 1M -> read:write ratio and rate of object overwrite/replacement are important factors
        - more replacement can lead to fragmentation, in which case filesystem can be more performant
        - more info: https://www.microsoft.com/en-us/research/publication/to-blob-or-not-to-blob-large-object-storage-in-a-database-or-a-filesystem/
      - larger than 1M -> filesystem
    - Other considerations
      - Database objects are loaded entirely into memory: inefficient to stream a file from a database
      - Replication will be slow if database table rows are large objects
  - **Replication**
    - Replication: making copies of data, called replicas, and storing them on different nodes.
    - Partitioning and sharing are both about dividing a data set into subsets.
      - Sharding implies the subsets are distributed across multiple nodes
      - Partitioning does not.
    - **Motivations**
      - A single host has limitations, so it cannot fulfill requirements:
        - Fault-tolerance
          - backup of data to nodes within and across data centers, in case of node or network failure
          - define failover process for other nodes to take over roles and partitions/shards of failed nodes
        - Higher storage capacity
          - Vertical scaling with multiple large hard drives is expensive
          - A single node's throughput could become a bottleneck
        - Higher throughput
          - Process reads and writes for multiple simultaneous processes and users
        - Lower latency
          - Geographically distribute replicas closer to dispersed users
          - Increase number of replicas in a data center if there is more read traffic in that area
        - To scale reads: increase the number of replicas of that data.
        - To scale writes: more difficult; detailed below.
    - **Distributing replicas**
      - Typical design: one backup onto a host on the same rack and one backup on a host on a different rack or data center (or availability zone) or both
      - Data may also be sharded
        - Tradeoff: need to track the shards' locations
        - Benefits
          - Scale storage: if database/table too big to fit on one node, sharding allows it to remain a single logical unit
          - Scale memory: if DB stored in memory, vertical scaling of memory quickly becomes expensive ($)
          - Scale processing: sharded DB can take advantage of parallel processing
          - Locality: to reduce latency, shard such that the data a particular cluster node needs is likely to be stored locally rather than on another node
    - **Single-leader replication**
      - Purpose: scale reads, not writes.
      - SQL service loses ACID consistency due to inherent latency in the replication
      - Design
        - All write operations occur on single leader node
        - Reads from any of N followers
        - If leader fails, secondary leader promoted to leader
        - A single node has max throughput, so number of followers is also limited
          - Can improve this with pyramid-like multi-level approach; each level replicates to level below; consistency is further delayed
        - Limitations
          - Entire DB must fit on single host
          - Eventual consistency
      - Simplest to implement
      - **Scaling via query logic in app layer (hack)**
        - Use case: DB size > single host; cannot reduce DB size; we want to continue using SQL
        - partition the DB across multiple hosts
        - need additional metadata about where data is stored
        - application has to maintain query logic
    - **Multi-leader replication**
      - Purpose: scale writes and database storage size.
      - Design
        - Multiple nodes designated as leaders
        - Writes can be made to any leader
        - Each leader must replicate writes to all other nodes
      - Disadvantage: must handle race conditions, not present in single-leader replication
        - e.g., a row is simultaneously updated on one leader and deleted on another leader
          - using timestamps doesn't work because clocks are not perfectly synchronized across nodes
          - attempting to use same clock across services doesn't work due to **clock skew**
            - impossible to determine order if queries made to multiple nodes within time interval < clock skew
      - **Leaderless replication**
        - All nodes are equal
        - How to handle race conditions?
          - One solution: **quorum** — n/2 + 1 nodes agree on read/write operations
            - higher quorums provide better consistency but with increased latency
            - if we need consistency, we can trade off between writes and reads
              - for fast writes: set low write (W) quorum and high read (R) quorum
              - for fast reads: set high write quorum and low read quorum
              - to get strong consistency, **R + W > N**, where N is the number of nodes
                - otherwise, only eventual consistency possible, UPDATE and DELETE cannot be consistent
            - Strong consistency sacrifices performance and availability for up-to-date data, while eventual consistency offers better performance and availability but allows for temporary inconsistencies.
        - Examples: Cassandra, Dynamo, Riak, and Voldemort
  - **Scaling storage capacity with sharded databases**
    - Imposes limitations on SQL operations
      - JOIN queries are slower
      - JOINs on shard keys are faster
      - Aggregation operations involve both DB and app logic
        - Sum and mean would be easier (implement by each node returning sum and count)
        - Median and percentile would be more complicated and slower
    - See also: https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/
  - **Aggregating Events**
  - **Messaging**
    - Use to smooth out uneven traffic, prevent a service from being overloaded by traffic spikes